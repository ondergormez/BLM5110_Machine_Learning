# 2. Week - 7 October 2024 Monday

Bu hafta karar ağaçları ile ilgili konuları işleyeceğiz.

Strategies of a machine learning model:

Real World -> Measuring Device -> Preprocessing -> Feature Selection -> Model Selection -> Model Training -> Model Evaluation -> Model Deployment

TODO: Bu dersin sonunda entropi (entropy) ve bilgi kazancı (information gain) hesaplamayı öğren.

Data toplarken genel bir sistem analizi yapıp, gözle gördüğünüzün dışında da data toplamanız gerekmektedir.
* Erişebildiğiniz ve lazım olup olmadığından emin olmadığınız her şeyi toplayabilirsiniz.
* Bu bilgileri feature extraction ile zaten gerekirse drop edeceğiz.
* Principal Component Analysis: En çok kullanılan özellik azaltma yöntemlerinden biridir.
  * Eigen Value ve Eigen Vector yani matrisin önemli olan özellikleri üzerinden yüz tanımayı yapalım denmiş. Sonrasında yüz tanımanın başarımı artmış.
  * Eskiden göz, burun, ağız gibi yüzün ayırt edici kısımlarının koordinatlarının bir birine mesafesine bakılarak yüz tanıma yapılmaya çalışılıyordu. Bu çok başarılı bir yöntem değildi.

# Unsupservised Learning
* Çoğu zaman bizde sınıfı bilmiyoruz. 
* Bazende milyonlarca veya milyarlarca veri var ve bunları manuel etiketlememiz mümkün değil.
* Temelde yapılan iş aslında veriyi özelliklerine göre gruplandırmak.
* Kilo, boy ve yaş grubu verilen bir veri setinde kız veya erkek çocuk olup olmadığını tahmin etmek.
  * Veriye bakarak boyu çok uzunsa ve kilosu çok fazlaysa erkek çocuk olma ihtimali yüksektir gibi bir çıkarım ortaya çıkabilir. Fakat bu %100 doğru bir çıkarım olmayacaktır. Çok uzun kızlar da olabilir. Amacımız verinin genelini temsil eden bir model oluşturmak. Özel istisnalarla ilgilenmiyoruz.
  * Bu özellikler ülkeden ülkeye değişebilir. Örneğin Türkiye'de 1.70 boyu olan bir boy kızlar için çok uzun iken, farklı bir ülkede sonuç farklı olabilir. Verilen veri setine göre bir model oluşturulur.


D:<x>  
x: input features

# Supervised Learning
Inputlarımız datalar, Elif'in ve Ayşe'nin yüzü şeklinde etiketli.  
Outputlarımız ise etiketler, Bu yeni fotoğraftaki kişi Elif veya Ayşe dir bilgisi.

D:<x, y>  
x: input features  
y: output label  

* Bugün supervised learning in içindeki karar ağaçlarını (decision trees) işleyeceğiz.
* Örnek sayısının çok fazla olmadığı zamanlarda karar ağaçları çok kullanışlı ve iyi sonuçlar veren bir yöntemdir.

İki farklı yöntem

**Regression**: Sürekli zamanda bir bilgi ediniliyor. İleriye yönelik bir tahmin yapmaya çalışıyoruz gibi düşünebiliriz.
* Geçtiğimizi 10 yılda alınan yağış şu kadar iken fındık 100 ton du. 2024 yılında xyz yağış yağdı, kaç ton fındık alınabilir?
* Ocak ayından bu yana beşiktaş ta ev fiyatları şu kadardı. Aralık ayına geldiğimde fiyat ne kadar olacak?

**Classification**:
* Kedidir, köpektir gibi sınıflandırıyoruz.


Two Approaches;  
**Discriminative Classifiers**:
* Decision Boundry (Karar sınırı): Sınıflandırma yaparken sınıfların birbirinden ayrıldığı çizginin yerini belirlememiz lazım.
* Spam not spam sınıflandırması yaparken spam ve not spam arasında bir çizgi çizmemiz gerekiyor.

**Generative Classifiers**:
* Bayes Classifier
* Verilen bilgilerden örneğin hangi sınıfa ait olabileceği bilgisini çıkartıyorlar. %80 A sınıfı %20 B sınıfı gibi.

En temellerinden bir tanesi Binary Classification. Sadece 2 sınıfa ayırma yapılır. Spam ve not spam gibi.  
Multi-Class Classification: Birden fazla sınıf varsa. Köpeğin türü, labrador, golden retriever, rado gibi.  

Decision Stamp: Tek bir özellikle sonuca varabiliyorsanız budur.

## Desicion Trees
2 önemli avantajı vardır.
* En önemli özelliği embedded feature selection yapıyor olmasıdır. 
  * Ağacı oluştururken gereksiz özellikleri kendisi eliyor.
  * Ayrı bir özellik seçme algoritması kullanmamıza gerek yok.
  * Information Gain bu işe yarıyor. En önemli özellikler ön plana çıkıyor.
  * Ağacı oluşturduktan sonra altta bulunan node ları  keserek (prune ederek) bilgi kazancı tekrar hesaplanır
    * Ağacın daha iyi mi sonuç verdiğine bakılır.
    * İyi sonuç verdiyse bu şekliyle desicion tree oluşturulmuş olur.
* Karar sınırları çok esnektir. Yamuk yumuk olabilir. Sınıf başarısını pozitif etkilemektedir.


* Öğrenme karar ağacının oluşturulmasıdır
* Karar ağacını oluşturduktan sonra yeni gelen dataya bakıyorum ve karar ağacında ilerleyerek karar veriyoruz.
  * Hangi sınıfa ait olduğuna karar verilir (Classification)
  * Geleceğe yönelik tahmin olabilir (Regression)

Sayısal değerler olduğunda ne yapacağız?
* Bir eşik seviyesi kullanmalıyız
* Yaşı 40'dan büyükse şunu yap, küçükse bunu yap gibi.
* Eşik seviyesini (threshold) belirlemek çok önemli.
* Karar ağaçlarının en güzel özelliklerinden biri sparse data ile çalışabilmesidir.

Desicion tree yi programatic olarak kodlamak istersek aşağıdakine benzer bir durum ortaya çıkar;
* Aslında klasik programlama gibi olmuş

if outlook = sunny AND humidity = normal  
OR  
....  
then  
    play tennis  

### Ağacı Nasıl Oluşturacağımıza Nasıl Karar Vereceğiz?
* Köke ne koyduğumuz ağacın uzunluğunu (derinliğini) çok fazla etkiler.
* Non deterministic polinomial bir denklemdir. En iyi ağacı oluşturmak çok mümkün değil.
* Bu nedenle pruning yaparak ve farklı özellikleri kök düğüme koyarak denemeler yapmamız gerekiyor.
* Bölen özelliğimizin olabildiğince saf bir sınıf olmasına dikkat etmeliyiz.
* Saf olan özelliği bulmak için entropy hesabı yapılması gerekmektedir. Entropy sıfıra yaklaştıkça saflık artar. 0 iken özellik tamamen saftır.

Karar ağacını oluştururken entropileri de kullanarak bilgi kazancı hesabı yapıyoruz. Bilgi kazancı en yüksek olan özelliği kök olarak seçiyoruz.
* Alt node lardaki hesaplamalarda benzer şekilde yapılıyor.

### Entropy
Karar ağacına hangi özelliği root düğüm olarak yerleştireceğimize karar vermede kullanılır.

### Information Gain (Bilgi Kazancı)
Her alt kümenin entropisi hesaplanır.  
Tüm veri setinin etropisinden çıkarılarak bilgi kazancı hesaplanır.


### Overfitting in Decision Trees
* Training de %90 başarı, testte %85 başarı varsa bu durum normaldir.
* Training de %90 başarı, testte ise %60 başarı varsa bu durum overfitting olabilir.
* Training set çok küçükse overfitting oluşabilir
* Pruning: Ağacı kesmek ve alt ağaçları ortadan kaldırmak anlamına geliyor. Prune ederken amacımız test ve validasyon datasında daha doğru sonuç elde etmektir.
* Çözüm
  * Reduced error pruning
  * Early stopping
  * Rule post pruning
