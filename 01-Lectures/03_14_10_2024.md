# 3. Week - 14 October 2024 Monday

Lineer Regression konusu ile başlıyoruz.

# Linear Regression
Lineer bir doğru uydurarak bilinmeyenleri tahmin etmeye çalışmaktır.
* Doğru örneklerin tam ortasından geçerek tüm veri setini temsil edebilecek şekilde olmalıdır. Bu tam olarak mümkün değil aslında.

![](https://media.geeksforgeeks.org/wp-content/uploads/20231129130431/11111111.png)

# Lineer Regression Error

* Öğrenme hatayı minimize edecek şekilde ağırlıkların değiştirilmesi demektir.
* Ağırlığı çok küçük seçersem iterasyon sayısı çok artarak global minimum a ulaşabiliyorum.
* Ağırlığı çok büyük seçersem iterasyon sayısı azalır fakat, global minimum a yaklaşamadan üzerinden pass geçebilirim.

![](https://miro.medium.com/v2/resize:fit:1400/1*tQTcGTLZqnI5rp3JYO_4NA.png)

# Gradient Descent Algorithm (Iterative Local Optimization Algorithm)

* Gradient: Derivative of a function that has more than one input feature
* Aim: Find optimal weights that minimize the cost function
* Başlangıçta büyük adımlarla başlar ve sona yaklaştıkça eğim azalacağı için küçük adımlarla ilerler.

* Learning Rate başlangıçta büyük alınarak, zaman arttıkça küçültülebilir.

## GD Algorithm
1 EPOCH: Tüm veri setini bir kere görmek anlamına gelir.

* Begin training with randomized weights and biases near zero +-0.5
   REPEAT
    * STEP 1: Calculate the loss
    * Determine the **direction to move** the weights to **reduce the loss**
    * More the weights in that direction
    * Return to Step 1 UNTIL CONVERGENCE

Convergence: minimum cost function value.  
Convergence olana kadar bizim iterasyonlarımız devam edilebilir veya belirli bir iterasyon sayısına ulaşılınca durdurulabilir.

## Stochastic Gradient Descent (Online Linear Regression)
* Online linear regression da deniyor.

3 tane gradient descent türü işleyeceğiz.
* Stochastic Gradient Descent
* Batch Gradient Descent
* Mini Batch Gradient Descent

Algoritma temel olarak aşağıdaki şekilde olacak:
* Her bir örnek için çıkışı hesapla
* Hesapladığın çıkış ile beklediğin çıkış arasındaki farkı hesapla
* Bu farkı minimize etmek için ağırlıkları güncelle

### SGD Algorithm

* Her örnek için ağırlıkları ayrı ayrı hesaplayıp ağırlıkları güncelliyoruz. Bu çok maliyetli bir işlem.

* Initialize weights
* Set learning rate  
  REPEAT
    * STEP 1: Select a sample <xi, yi> from D
    * Calculate the loss
    * Update weights vector for feature Xj
    * wj(t + 1) = wj(t) - η * (yi - yi^) * xi
    * Return to Step 1 UNTIL CONVERGENCE

### Batch Gradient Descent

* Tüm veri setini bir kere görüp hatayı hesaplıyoruz. (1 Epoch ilerlemek anlamına gelir)
* Sonra ağırlıkları güncelliyoruz.
* Tekrar tüm veri setini görüp hatayı hesaplıyoruz.
* Ağırlıkları güncelliyoruz.

### Mini Batch Gradient Descent
* Tüm örnekleri görmek yerine, örneklerin alt kümesini alıp işlem yaparız.
* 1000 örenek varsa, mini batch size 100 örnek seçildiyse, 10 tane mini batch olur.
* 1 Epoch için 10 iterasyon gitmemiz gerekir.

* Bunda 100 örnek seçip, hatayı hesaplıyoruz.
* Sonra ağırlıkları güncelliyoruz.
* Tekrar 100 örnek seçip, hatayı hesaplıyoruz.
* Ağırlıkları güncelliyoruz.
* Bu şekilde devam eder.

# Normalizations

## Min-Max Scaling
Bunun sorunu aykırı değerlerin (outlier) öncesinde discard edilmesi gerekmektedir.
* Yoksa outlier nedeniyle örneklerin scale i çok düşürülmüş olur.
* Mesela sıcaklık için 1 tane 90 Derece olarak ölçüm yapılmış. Diğer örneklerin aralığı 0 - 30 derece arasıysa bunu normalize ederken önemli verilerin distance farkını ignore etmiş oluyoruz.

## Standard Deviation Normalization (Z-Score Normalization)
* Values are centered around mean. Böylelikle aykırı (outlier) değerlerin etkisi daha da azaltılmış olur. 
* Xi normalize edilmemişse, X in büyük değerlerinden ötürü ağırlık hoplaya zıplaya gidiyor.
